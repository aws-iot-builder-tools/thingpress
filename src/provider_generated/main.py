"""
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

Lambda function to process certificate files generated by the generate_certificates.py script
and begin the import processing pipeline
"""
import base64
import json
import os
from typing import Any, Dict

from aws_lambda_powertools import Logger
from aws_lambda_powertools.utilities.data_classes import SQSEvent
from aws_lambda_powertools.utilities.idempotency.config import IdempotencyConfig
from aws_lambda_powertools.utilities.idempotency.persistence.dynamodb import \
    DynamoDBPersistenceLayer
from aws_lambda_powertools.utilities.typing import LambdaContext
from boto3 import Session
from layer_utils.aws_utils import s3_object_bytes
from layer_utils.cert_utils import get_cn
from layer_utils.throttling_utils import create_standardized_throttler

# Initialize Logger and Idempotency
logger = Logger(service="provider_generated")
default_session: Session = Session()

if os.environ.get("POWERTOOLS_IDEMPOTENCY_TABLE") is None:
    raise ValueError("Environment variable POWERTOOLS_IDEMPOTENCY_TABLE not set.")
POWERTOOLS_IDEMPOTENCY_TABLE: str = os.environ["POWERTOOLS_IDEMPOTENCY_TABLE"]
if os.environ.get("POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS") is None:
    POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS: int = 3600
POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS: int = int(
    os.environ.get("POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS", 3600))

# Initialize persistence layer for idempotency
persistence_layer = DynamoDBPersistenceLayer(
    table_name=POWERTOOLS_IDEMPOTENCY_TABLE,
    key_attr="id",
    expiry_attr="expiration",
    status_attr="status",
    data_attr="data",
    validation_key_attr="validation"
)

# Configure idempotency
idempotency_config = IdempotencyConfig(
    expires_after_seconds=POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS
)

def file_key_generator(event, _context):
    """Generate a unique key based on S3 bucket and key"""
    if isinstance(event, dict) and "bucket" in event and "key" in event:
        # Use bucket and key as the idempotency key
        return f"{event['bucket']}:{event['key']}"
    return None

#@idempotent_function(
#    persistence_store=persistence_layer,
#    config=idempotency_config,
#    event_key_generator=file_key_generator,
#    data_keyword_argument="config"
#)

def process_certificate_file(config: Dict[str, Any], queue_url: str,
                             session: Session=default_session) -> int:
    """
    Process a file containing base64-encoded certificates (one per line) using batch processing.
    
    Args:
        config: Configuration dictionary with bucket and key information
        queue_url: URL of the target SQS queue
        session: AWS session to use
        
    Returns:
        Number of certificates processed
    """
    logger.info({
        "message": "Processing certificate file",
        "bucket": config['bucket'],
        "key": config['key']
    })

    # Get the file content from S3
    file_content = s3_object_bytes(config['bucket'], config['key'], getvalue=True,
                                   session=session).decode()

    # Process certificates in batches for optimal SQS throughput
    batch_messages = []
    batch_size = 10  # SQS batch limit

    # Initialize standardized throttler
    throttler = create_standardized_throttler()
    total_count = 0

    for line in file_content.splitlines():
        line = line.strip()
        if not line:
            continue

        # Create a copy of the config for this certificate
        cert_config = config.copy()

        # Store the certificate
        cert_config['certificate'] = line
        cert_bytes = base64.b64decode(line)
        cert_config['thing'] = get_cn(cert_bytes)

        batch_messages.append(cert_config)
        total_count += 1

        # Send batch when full
        if len(batch_messages) >= batch_size:
            throttler.send_batch_with_throttling(batch_messages, queue_url, session)
            batch_messages = []

    # Send remaining messages
    if batch_messages:
        throttler.send_batch_with_throttling(
            batch_messages, queue_url, session, is_final_batch=True)

    # Get throttling statistics for logging
    throttling_stats = throttler.get_throttling_stats()

    logger.info({
        "message": "Processed certificates from file with standardized throttling",
        "total_certificates": total_count,
        "total_batches": throttling_stats["total_batches_processed"],
        "api_calls_saved": total_count - throttling_stats["total_batches_processed"],
        "throttling_stats": throttling_stats,
        "bucket": config['bucket'],
        "key": config['key']
    })

    return total_count

def lambda_handler(event, _context: LambdaContext) -> dict:
    """
    Process certificate files generated by generate_certificates.py from SQS messages and
    forward to target queue.
    
    This Lambda function processes SQS messages containing S3 bucket and object information
    for certificate files generated by the generate_certificates.py script. For each file:
    1. Retrieves the file containing base64-encoded certificates (one per line) from S3
    2. For each certificate in the file:
       a. Extracts the Common Name (CN) from the certificate to use as the Thing name
       b. Forwards the certificate data and Thing name to the target SQS queue
    
    The certificate files are expected to contain one base64-encoded certificate per line,
    with each certificate including its full chain.
    
    Environment variables:
        QUEUE_TARGET: URL of the SQS queue to forward processed certificates to
    
    Args:
        event (SQSEvent): SQS event containing messages with S3 bucket/object information
        context (LambdaContext): Lambda execution context (unused)
        
    Returns:
        dict: The original event for AWS Lambda SQS batch processing
    """
    # Handle both raw dict and SQSEvent object formats
    if hasattr(event, 'records'):
        # SQSEvent object format
        sqs_event = event
        raw_event = event.raw_event
    else:
        # Raw dict format - convert to SQSEvent
        sqs_event = SQSEvent(event)
        raw_event = event

    queue_url = os.environ['QUEUE_TARGET']
    total_processed = 0

    for record in sqs_event.records:
        config = json.loads(record.body)
        logger.info({
            "message": "Processing SQS message",
            "bucket": config.get('bucket'),
            "key": config.get('key')
        })
        total_processed += process_certificate_file(config, queue_url)

    logger.info({
        "message": "Total certificates processed",
        "count": total_processed
    })

    return raw_event
