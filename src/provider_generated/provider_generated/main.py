"""
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

Lambda function to process certificate files generated by the generate_certificates.py script
and begin the import processing pipeline
"""
import os
import json
import base64
from typing import Dict, Any
from aws_lambda_powertools import Logger
from aws_lambda_powertools.utilities.typing import LambdaContext
from aws_lambda_powertools.utilities.data_classes import SQSEvent
from aws_lambda_powertools.utilities.idempotency import idempotent_function
from aws_lambda_powertools.utilities.idempotency.persistence.dynamodb import DynamoDBPersistenceLayer
from aws_lambda_powertools.utilities.idempotency.config import IdempotencyConfig
from layer_utils.aws_utils import s3_object_bytes, send_sqs_message
from layer_utils.cert_utils import get_cn
from boto3 import Session

# Initialize Logger and Idempotency
logger = Logger(service="provider_generated")
default_session: Session = Session()

if os.environ.get("POWERTOOLS_IDEMPOTENCY_TABLE") is None:
    raise ValueError("Environment variable POWERTOOLS_IDEMPOTENCY_TABLE not set.")
POWERTOOLS_IDEMPOTENCY_TABLE: str = os.environ["POWERTOOLS_IDEMPOTENCY_TABLE"]
if os.environ.get("POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS") is None:
    POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS: int = 3600
POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS: int = int(
    os.environ.get("POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS", 3600))

# Initialize persistence layer for idempotency
persistence_layer = DynamoDBPersistenceLayer(
    table_name=POWERTOOLS_IDEMPOTENCY_TABLE,
    key_attr="id",
    expiry_attr="expiration",
    status_attr="status",
    data_attr="data",
    validation_key_attr="validation"
)

# Configure idempotency
idempotency_config = IdempotencyConfig(
    expires_after_seconds=POWERTOOLS_IDEMPOTENCY_EXPIRY_SECONDS
)

def file_key_generator(event, context):
    """Generate a unique key based on S3 bucket and key"""
    if isinstance(event, dict) and "bucket" in event and "key" in event:
        # Use bucket and key as the idempotency key
        return f"{event['bucket']}:{event['key']}"
    return None

#@idempotent_function(
#    persistence_store=persistence_layer,
#    config=idempotency_config,
#    event_key_generator=file_key_generator,
#    data_keyword_argument="config"
#)

def process_certificate_file(config: Dict[str, Any], queue_url: str,
                             session: Session=default_session) -> int:
    """
    Process a file containing base64-encoded certificates (one per line).
    
    Args:
        config: Configuration dictionary with bucket and key information
        queue_url: URL of the target SQS queue
        session: AWS session to use
        
    Returns:
        Number of certificates processed
    """
    logger.info({
        "message": "Processing certificate file",
        "bucket": config['bucket'],
        "key": config['key']
    })

    # Get the file content from S3
    file_content = s3_object_bytes(config['bucket'], config['key'], getvalue=True,
                                   session=session).decode()

    # Process each line (each line is a base64-encoded certificate)
    count = 0
    for line in file_content.splitlines():
        line = line.strip()
        if not line:
            continue

        # Create a copy of the config for this certificate
        cert_config = config.copy()

        # Store the certificate
        cert_config['certificate'] = line
        cert_bytes = base64.b64decode(line)

        cert_config['thing'] = get_cn(cert_bytes)

        # Send to the target queue
        send_sqs_message(cert_config, queue_url, session)
        count += 1

    logger.info({
        "message": "Processed certificates from file",
        "count": count,
        "bucket": config['bucket'],
        "key": config['key']
    })

    return count

def lambda_handler(event, context: LambdaContext) -> dict: # pylint: disable=unused-argument
    """
    Process certificate files generated by generate_certificates.py from SQS messages and
    forward to target queue.
    
    This Lambda function processes SQS messages containing S3 bucket and object information
    for certificate files generated by the generate_certificates.py script. For each file:
    1. Retrieves the file containing base64-encoded certificates (one per line) from S3
    2. For each certificate in the file:
       a. Extracts the Common Name (CN) from the certificate to use as the Thing name
       b. Forwards the certificate data and Thing name to the target SQS queue
    
    The certificate files are expected to contain one base64-encoded certificate per line,
    with each certificate including its full chain.
    
    Environment variables:
        QUEUE_TARGET: URL of the SQS queue to forward processed certificates to
    
    Args:
        event (SQSEvent): SQS event containing messages with S3 bucket/object information
        context (LambdaContext): Lambda execution context (unused)
        
    Returns:
        dict: The original event for AWS Lambda SQS batch processing
    """
    # Handle both raw dict and SQSEvent object formats
    if hasattr(event, 'records'):
        # SQSEvent object format
        sqs_event = event
        raw_event = event.raw_event
    else:
        # Raw dict format - convert to SQSEvent
        from aws_lambda_powertools.utilities.data_classes import SQSEvent
        sqs_event = SQSEvent(event)
        raw_event = event
    
    queue_url = os.environ['QUEUE_TARGET']
    total_processed = 0

    for record in sqs_event.records:
        config = json.loads(record.body)
        logger.info({
            "message": "Processing SQS message",
            "bucket": config.get('bucket'),
            "key": config.get('key')
        })
        total_processed += process_certificate_file(config, queue_url)

    logger.info({
        "message": "Total certificates processed",
        "count": total_processed
    })
    
    return raw_event
