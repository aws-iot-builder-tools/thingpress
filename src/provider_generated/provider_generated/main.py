"""
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

Lambda function to process certificate files generated by the generate_certificates.py script
and begin the import processing pipeline
"""
import base64
import json
import os
from typing import Any, Dict

from aws_lambda_powertools import Logger
from aws_lambda_powertools.utilities.typing import LambdaContext
from aws_lambda_powertools.utilities.data_classes import SQSEvent
from boto3 import Session
from layer_utils.aws_utils import s3_object_bytes
from layer_utils.cert_utils import get_cn
from layer_utils.throttling_utils import create_standardized_throttler
from layer_utils.aws_utils import powertools_idempotency_environ

# Initialize Logger and Idempotency
logger = Logger(service="provider_generated")
default_session: Session = Session()

persistence_layer, idempotency_config = powertools_idempotency_environ()

def file_key_generator(event, _context):
    """Generate a unique key based on S3 bucket and key"""
    if isinstance(event, dict) and "bucket" in event and "key" in event:
        # Use bucket and key as the idempotency key
        return f"{event['bucket']}:{event['key']}"
    return None

#@idempotent_function(
#    persistence_store=persistence_layer,
#    config=idempotency_config,
#    event_key_generator=file_key_generator,
#    data_keyword_argument="config"
#)

def process_certificate_file(config: Dict[str, Any], queue_url: str,
                             session: Session=default_session) -> int:
    """
    Process a file containing base64-encoded certificates (one per line).
    
    Args:
        config: Configuration dictionary with bucket and key information
        queue_url: URL of the target SQS queue
        session: AWS session to use
        
    Returns:
        Number of certificates processed
    """
    logger.info({
        "message": "Processing certificate file",
        "bucket": config['bucket'],
        "key": config['key']
    })

    # Get the file content from S3
    file_content = s3_object_bytes(config['bucket'],
                                   config['key'],
#                                   getvalue=True,
                                   session=session).decode()

    # Process certificates in batches for optimal SQS throughput
    batch_messages = []
    batch_size = 10  # SQS batch limit
    count = 0

    # Initialize standardized throttler
    throttler = create_standardized_throttler()

    for line in file_content.splitlines():
        line = line.strip()
        if not line:
            continue

        # Create a copy of the config for this certificate
        cert_config = config.copy()

        # Store the certificate
        cert_bytes = base64.b64decode(line)
        cert_config['thing'] = get_cn(cert_bytes)
        cert_config['certificate'] = str(base64.b64encode(cert_bytes))
        # Convert bytes to string for get_cn function
        #cert_string = cert_bytes.decode('utf-8')
        #cert_config['thing'] = get_cn(cert_string)

        # Add to batch
        batch_messages.append(cert_config)
        count += 1

        # Send batch when full
        if len(batch_messages) >= batch_size:
            throttler.send_batch_with_throttling(batch_messages, queue_url, session)
            batch_messages = []

    # Send remaining messages in final batch
    if batch_messages:
        throttler.send_batch_with_throttling(
            batch_messages, queue_url, session, is_final_batch=True)

    # Get throttling statistics for logging
    throttling_stats = throttler.get_throttling_stats()

    logger.info({
        "message": "Processed certificates from file with standardized throttling",
        "count": count,
        "throttling_stats": throttling_stats,
        "bucket": config['bucket'],
        "key": config['key']
    })

    return count

def lambda_handler(event: dict, context: LambdaContext) -> dict: # pylint: disable=unused-argument
    """
    Process certificate files generated by generate_certificates.py from SQS messages and
    forward to target queue.
    
    This Lambda function processes SQS messages containing S3 bucket and object information
    for certificate files generated by the generate_certificates.py script. For each file:
    1. Retrieves the file containing base64-encoded certificates (one per line) from S3
    2. For each certificate in the file:
       a. Extracts the Common Name (CN) from the certificate to use as the Thing name
       b. Forwards the certificate data and Thing name to the target SQS queue
    
    The certificate files are expected to contain one base64-encoded certificate per line,
    with each certificate including its full chain.
    
    Environment variables:
        QUEUE_TARGET: URL of the SQS queue to forward processed certificates to
    
    Args:
        event (SQSEvent): SQS event containing messages with S3 bucket/object information
        context (LambdaContext): Lambda execution context (unused)
        
    Returns:
        dict: The original event for AWS Lambda SQS batch processing
    """
    # Convert raw dict to SQSEvent (AWS always sends raw dict to Lambda handlers)
    sqs_event = SQSEvent(event)

    queue_url = os.environ['QUEUE_TARGET']
    total_processed = 0

    for record in sqs_event.records:
        config = json.loads(record.body)
        logger.info({
            "message": "Processing SQS message",
            "bucket": config.get('bucket'),
            "key": config.get('key')
        })
        total_processed += process_certificate_file(config, queue_url)

    logger.info({
        "message": "Total certificates processed",
        "count": total_processed
    })

    return event
